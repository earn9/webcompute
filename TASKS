NEXT STEPS
    - Frame sync
    - Interactive AOBench

TASKS
    [] Interactive render
        [] Frame sync
        [] Inputs that change according to UI state
            [] Time
            [] Time delta
            [] Mouse / Touch
                [] Position
                [] Buttons
                [] Click
                [] Drag

    [] Distribution network
        [] Latency limit to restrict node selection
        [] Job distribution network
        [] Reducer network

    [] Target optimization
        [] Either figure out
            - how to make GLSL AOBench perf match ISPC
            - how to pass separate kernels to different targets
                [] float16 not supported on ISPC
                [] float64/int64 not supported on mobiles
        [] Latency minimization
            [x] Pipeline all requests at the beginning
            [] Pipeline multiple requests to maximize throughput (& minimize avg req latency)
                - If a node takes 5 ms for 1 work unit and 15 ms for 11 work units, work unit cost is 1 ms, pipeline wait 4 ms => enqueue 12 work units to hit 16 ms latency target (12 * 1 + 4 = 16)
                - If a node takes 5 ms for 1 work unit and 45 ms for 11 work units, work unit cost is 4 ms, pipeline wait 1 ms => enqueue 3 work units to hit 16 ms latency target (3 * 4 + 1 = 13)
                [] Time work unit runtime
                [] Measure work unit cost & scaling
            [] Async I/O to overlap fwrite and kernel execution

    [] ARM Linux ports
        [] Run on webcam
        [] Run on RC car
        [] Run on light switch

    [] Persistent runtime
        [x] Long-running process for running ISPC with different inputs
            [x] Not very important, ISPC startup time is quite minimal
            [x] Keep process up and receive new inputs via socket
        [x] Long-running process for running Vulkan jobs
            [x] Start process on receiving first job
            [x] Keep latest shader pipeline + buffer alloc cached
            [x] Receive new inputs via socket
            [] Receive varying input & output lengths

    [] Adaptive job size (jobSize = jobThroughput * (latencyDeadline - jobStartOverhead))
        - jobThroughput = jobSize / (computeTime + transferTime - jobStartOverhead)
        - e.g. If you're rendering a 10 MB 1080p frame and it takes a node 11 ms to render the frame 
                and 90 ms to transfer it, and there's a 1 ms overhead to starting a new job:
                jobThroughput = 10 MB / (0.011s + 0.09s - 0.001s) = 10 MB / 0.1 s = 100 MB/s
                If you want to render at 60Hz, your latencyDeadline is around 11 ms. Then the
                best job size for the node would be:
                jobSize = 100 MB/s * (11 ms - 1 ms) = 100 MB/s * 0.01s = 1 MB
        - e.g. Suppose you JPEG compress the frames that you send out at 1:20 compression ratio,
                and the compression takes 5 ms. Add 5 ms to compute time, divide transfer time by 20:
                jobThroughput = 10 MB / (0.016s + 0.0045s - 0.001s) = 10 MB / 0.02s = 500 MB/s
                Now the best job size for the node would be:
                jobSize = 500 MB/s * (11 ms - 1 ms) = 5 MB
        [] Measure node
            [] Job throughput for kernel
            [] Job overhead
        [] Compression transfer filter
        [] Adapt workgroup size to match throughput
            [] Calculate workgroup size for output size
            [] Split frame into tiles to match output sizes

    [x] Windows port
        [x] Run on the Dell laptop /w Win10
        [x] Run on the i7-3770k

    [x] Termux port
        [x] Run on Nokia 8
        [x] Run on Note 5
        [x] Run on Tab S3
        [x] Run on Mate 20

    [x] GLSL Compute Shader runner
        [x] Compile GLSL shader to SPIRV
        [x] Vulkan runner app
            [x] Compile makefile
            [x] runner.cpp -like semantics
        [x] ISPC runner app
            [x] Compile GLSL shader to ISPC
            [x] Add launch[x,y,z] to spirv-to-ISPC output
            [x] Add runner.cpp -like semantics
        [x] GLSL backend target
            [x] Vulkan device enumeration in node info
            [x] /build instance
            [x] /new instance
        [x] Front-end
            [x] Send GLSL shader to ISPC and Vulkan targets: 
                [x] /new {language: 'glsl', target: 'vulkan', device: 0}
                [x] /new {language: 'glsl', target: 'ispc'}
                - That's... it?

    [x] fix ISPC output when using enumerations of:
        in uvec3 gl_NumWorkGroups;
        in uvec3 gl_WorkGroupID;
        in uvec3 gl_LocalInvocationID;
        in uvec3 gl_GlobalInvocationID;
        in uint  gl_LocalInvocationIndex;
        - Monkey-patch "static SPIRV_INLINE void program_ispc_main(.*uniform struct outputs& (\S+),\s*uniform struct inputs& (\S+))"
          to "static SPIRV_INLINE void program_ispc_main(uniform int3 gl_NumWorkGroups, uniform int3 gl_WorkGroupID, varying int3 gl_LocalInvocationID, varying int3 gl_GlobalInvocationID, varying int gl_LocalInvocationIndex, uniform struct inputs& \2, uniform struct outputs& \1)"
        - Use new omni-parameterized call in ispcRunner.cpp
